{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "\n",
    "# Load the .npy file\n",
    "data = np.load(r'H:\\DDGAN\\figure\\uvw_18.npy')\n",
    "\n",
    "u = data[0]\n",
    "v = data[1]\n",
    "w = data[2]\n",
    "\n",
    "savemat(r'H:\\DDGAN\\figure\\u', {'data': u})\n",
    "savemat(r'H:\\DDGAN\\figure\\v', {'data': v})\n",
    "savemat(r'H:\\DDGAN\\figure\\w', {'data': w})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "class CustomScaler:\n",
    "    def __init__(self):\n",
    "        self.u_min = float('inf')\n",
    "        self.u_max = float('-inf')\n",
    "    \n",
    "    def partial_fit(self, batch):\n",
    "        batch_min = np.min(batch)\n",
    "        batch_max = np.max(batch)\n",
    "        self.u_min = min(self.u_min, batch_min)\n",
    "        self.u_max = max(self.u_max, batch_max)\n",
    "    \n",
    "    def fit(self, data_loader):\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            self.partial_fit(batch.numpy())\n",
    "            if i % 10 == 0:  # Print every 10 batches\n",
    "                print(f\"Batch {i}, Current min: {self.u_min:.6f}, Current max: {self.u_max:.6f}\")\n",
    "    \n",
    "    def transform(self, u):\n",
    "        u_scaled = (2 * u - (self.u_max + self.u_min)) / (self.u_max - self.u_min)\n",
    "        return np.clip(u_scaled, -1, 1)\n",
    "    \n",
    "    def untransform(self, u_scaled):\n",
    "        return 0.5 * (u_scaled * (self.u_max - self.u_min) + (self.u_max + self.u_min))\n",
    "\n",
    "class GANDataset(Dataset):\n",
    "    def __init__(self, folder_path, prefix):\n",
    "        self.folder_path = folder_path\n",
    "        self.prefix = prefix\n",
    "        self.num_timesteps = 5000  # 0 to 4999\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_timesteps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.folder_path, f'{self.prefix}_sample_{idx}.npy')\n",
    "        data = np.load(file_path)\n",
    "        return torch.from_numpy(data[0]).float()  # u = [0], v = [1] , w = [2]                       <--\n",
    "\n",
    "def create_dataloader(dataset, batch_size=32):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def save_scaler(scaler, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "def load_scaler(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def fit_scaler(gen_folder_path, disc_folder_path, scaler_path, batch_size = 5000):                     # <--\n",
    "    print(f\"Fitting scaler with batch size: {batch_size}\")\n",
    "    scaler = CustomScaler()\n",
    "\n",
    "    # Create DataLoaders\n",
    "    gen_dataset = GANDataset(gen_folder_path, 'g')\n",
    "    disc_dataset = GANDataset(disc_folder_path, 'd')\n",
    "\n",
    "    gen_loader = create_dataloader(gen_dataset, batch_size)\n",
    "    disc_loader = create_dataloader(disc_dataset, batch_size)\n",
    "\n",
    "    # Fit the single scaler on both datasets\n",
    "    print(\"Fitting scaler on generator data...\")\n",
    "    scaler.fit(gen_loader)\n",
    "    print(\"Fitting scaler on discriminator data...\")\n",
    "    scaler.fit(disc_loader)\n",
    "\n",
    "    # Save the scaler\n",
    "    save_scaler(scaler, scaler_path)\n",
    "    print(f\"Scaler saved to {scaler_path}\")\n",
    "    \n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:59: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:59: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\SLT2k\\AppData\\Local\\Temp\\ipykernel_72284\\3150293508.py:59: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  scipy.io.savemat('H:\\DDGAN\\data_distribution_w_before.mat', {'data_values': all_data_no_nan})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import pickle\n",
    "\n",
    "# Path to the mask file\n",
    "mask_path = r'H:\\DDGAN\\building_location.npy'\n",
    "scaler_path = r'H:\\DDGAN\\scaler_u.pkl'\n",
    "# Directory containing the .npy files\n",
    "directory_path = r'H:\\DDGAN\\Orig_dataset\\npy_uvw'\n",
    "\n",
    "# Load the mask\n",
    "mask = np.load(mask_path)\n",
    "\n",
    "# Ensure the mask is boolean\n",
    "mask = mask.astype(bool)\n",
    "\n",
    "# Load the scaler\n",
    "with open(scaler_path, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Collect all data\n",
    "all_data = []\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.npy'):\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "        data = np.load(filepath)\n",
    "        \n",
    "        # Check if the data is 3D (multiple matrices)\n",
    "        if data.ndim != 3:\n",
    "            print(f\"Unexpected data shape in {filename}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Ensure the first matrix and mask have the same shape\n",
    "        if data[2].shape != mask.shape:\n",
    "            print(f\"Shape mismatch for first matrix in {filename}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Apply the mask only to the first matrix\n",
    "        data[2][mask] = np.nan\n",
    "        \n",
    "        # Save the modified data back to the file\n",
    "        np.save(filepath, data)\n",
    "        \n",
    "        # Collect the data for the distribution (only from the first matrix)\n",
    "        all_data.extend(data[2].flatten())\n",
    "\n",
    "# Convert to a numpy array\n",
    "all_data = np.array(all_data)\n",
    "\n",
    "# Remove NaN values before normalization\n",
    "all_data_no_nan = all_data[~np.isnan(all_data)]\n",
    "\n",
    "# Normalize the data\n",
    "# normalized_data = scaler.transform(all_data_no_nan.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Save to .mat file for MATLAB\n",
    "scipy.io.savemat('H:\\DDGAN\\data_distribution_w_before.mat', {'data_values': all_data_no_nan})\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
